Last layer is not mentioned because Node = 10 and Activation = Softmax.

Layer	Node	kernel_Initializer	Activation	Epochs	Batch_Size 	Accuracy
1	100	“he_normal”		relu		100		512		20%	
2	100	“he_normal”		relu			

1	200	“he_normal”		relu		100		512		73%, 92%, 86%, 93%, 82%, 20%
2	100	“he_normal”		relu		

1	100	“he_normal”		relu		100		512		82%, 21%
2	200	“he_normal”		relu		


Activation = relu & relu, Accuracy fails.

1	100	“he_normal”		tanh		100		512		88%, 86%, 85%, 86%, 84%, 85%, 87% 88%: AVG: 86%
2	100	“he_normal”		tanh		

1	100	“he_normal”		tanh		100		512		86%, 85%, 86%, 87%, 85%, 88%, 88%, 87%: AVG: 86%
2	200	“he_normal”		tanh		

1	200	“he_normal”		tanh		100		512		86%, 84%, 86%, 86%,85%
2	100	“he_normal”		tanh		

Activation = tanh & tanh, Accuracy = 86%

1	100	“he_normal”		relu		100		512		85%, 87%, 86%, 87%, 86%, 88%, 89%, 86%, 87%: AVG: 86.78%
2	100	“he_normal”		tanh		

1	100	“he_normal”		tanh		100		512		88%,89%, 86%, 85%, 86%, 88%, 87%, 86%, 87%: AVG: 86.89%
2	100	“he_normal”		relu		

Activation = tanh & relu (and swap). Accuracy = 87%

1	25	“he_normal”		relu		100		512		85%, 87%, 86%, 87%, 84%, 85%, 86%, 88%, 87%, 87%: AVG: 86%
2	100	“he_normal”		tanh		

1	100	“he_normal”		relu		100		512		87%, 86%, 85%, 85%, 84%,87%, 86%,88%,85%,86%:AVG:85.9%
2	25	“he_normal”		tanh		

1	25	“he_normal”		tanh		100		512		86%,87%, 86%, 88%, 86%, 87%, 89%, 89%, 87%,86%: AVG: 87%
2	100	“he_normal”		relu		

1	100	“he_normal”		tanh		100		512		88%, 85%, 87%, 86%, 84%, 86%, 85%, 84%, 86%, 87%: AVG: 85.8%
2	25	“he_normal”		relu	

OPTIMAL:

1	25	“he_normal”		tanh		100		512		86%,87%, 86%, 88%, 86%, 87%, 89%, 89%, 87%,86%: AVG: 87%
2	100	“he_normal”		relu	
















